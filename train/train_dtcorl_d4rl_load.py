import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))
sys.path.append(parent_dir)

import torch
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import Dict, Any
from utils.network import Actor, Critic, Scalar
from utils.network import AutoEncoder, TRANS_Dynamic
from utils.tool import get_configs
from utils.dataset_env import make_delay_buffer_env_load, make_replay_buffer_env_load
from utils.replay_buffer import ReplayBuffer
# from utils.dataset import ReplayBuffer, DelayBuffer
from tqdm import tqdm, trange
from tensorboardX import SummaryWriter
import wandb
from rich import print
from copy import deepcopy
from collections import deque, defaultdict
import gym
import random, uuid

SYN_RATIO = 1

def construct_dict_from_tuple(data, device):
    data_dict = {}
    data_dict['obss'] = data[0].to(device)
    data_dict['actions'] = data[1].to(device)
    data_dict['rewards'] = data[2].to(device)
    data_dict['next_obss'] = data[3].to(device)
    data_dict['dones'] = data[4].to(device)
    return data_dict


class DTOCRL():
    def __init__(self, config):
        self.logger = SummaryWriter(config['exp_tag'])
        self.logger.add_text(
            "config",
            "|parametrix|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in config.items()])),
        )
        self.config = config
        # Initialize environment and offline dataset used for belief training in belief_buffer
        # env here has been wrapped with state mean and std normalization
        self.belief_buffer, self.env = make_delay_buffer_env_load(config['env'], config['delay'], config['batch_size'])
        print("============================ Initialized the DelayBuffer ============================")
        self.eval_env = deepcopy(self.env)
        observation_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.shape[0]
        action_high = float(self.env.action_space.high[0])
        action_low = float(self.env.action_space.low[0])
        self.dynamic_lr = config['lr_dynamic']
        # self.policy_lr = config['lr_policy']
        self.actor_lr = config['lr_actor']
        self.critic_lr = config['lr_critic']
        self.alpha_lr = config['lr_alpha']
        self.wandb = config['wandb']
        self.backup_entropy = config['backup_entropy']
        self.discount = config['gamma']
        self.soft_update_factor = config['soft_update_factor']
        self.device = config['device']

        # Initialize synthetic_data_buffer for storing synthetic data generated by latent dynamics/belief function model
        self.synthetic_data_buffer = ReplayBuffer(buffer_size=config['buffer_size'], observation_dim=observation_dim, action_dim=action_dim)
        print("============================ Initialized the Synthetic Buffer ============================")
        # Initialize replay_buffer for storing real data from offline dataset
        self.replay_buffer, _ = make_replay_buffer_env_load(config['env'], config['batch_size'])
        print("============================ Initialized the Replay Buffer ============================")

        # Initialize auto_encoder for encoding and decoding observation data
        self.auto_encoder = AutoEncoder(
            input_dim=observation_dim, 
            hidden_dim=256, 
            latent_dim=config['latent_dim']
        ).to(self.device)
        # Initialize latent_dynamic for predicting next latent state given current latent state and action
        self.latent_dynamic = TRANS_Dynamic(latent_dim=config['latent_dim'], 
            condition_dim=action_dim, 
            seq_len=config['delay'], 
            hidden_dim=config['latent_dim'],
            num_layers=config['num_layers'],
            num_heads=config['num_heads'],
        ).to(self.device)
        self.dynamic_optimizer = optim.Adam(list(self.latent_dynamic.parameters()) + list(self.auto_encoder.parameters()), lr=self.dynamic_lr)
        # Initialize actor and critic for policy learning
        self.actor = Actor(
            latent_dim=observation_dim, 
            action_dim=action_dim,
            action_high=action_high,
            action_low=action_low).to(self.device)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)

        self.critic_1 = Critic(latent_dim=observation_dim, action_dim=action_dim).to(self.device)
        self.target_1 = Critic(latent_dim=observation_dim, action_dim=action_dim).to(self.device)
        self.target_1.load_state_dict(self.critic_1.state_dict())
        self.target_1.eval()
        self.critic_2 = Critic(latent_dim=observation_dim, action_dim=action_dim).to(self.device)
        self.target_2 = Critic(latent_dim=observation_dim, action_dim=action_dim).to(self.device)
        self.target_2.load_state_dict(self.critic_2.state_dict())
        self.target_2.eval()
        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=self.critic_lr)
        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=self.critic_lr)

        # Load the pretrained models
        env_name = config['env'].split('-')[0]
        checkpoint_path = f"trans_reward/{env_name}_trans_Delay_{config['delay']}.pth"
        checkpoint = torch.load(checkpoint_path, map_location=torch.device(self.device))
        
        self.load_dyn_models(checkpoint)
        _ = self.rollout(config['num_rollout'])
        print(f"========================= Initialize Rollout =========================")
        self.reward_mean = checkpoint['reward_mean']
        self.reward_std = checkpoint['reward_std']
        self.belief_buffer.normalize_reward(self.reward_mean.cpu().item(), self.reward_std.cpu().item())
        self.replay_buffer.normalize_reward(self.reward_mean.cpu().item(), self.reward_std.cpu().item())
        print(f"========================= Normalize Reward =========================")

        # Initialize alpha for entropy regularization
        self.alpha = config['alpha']
        self.target_entropy = -np.prod(self.env.action_space.shape).item()
        self.use_automatic_entropy_tuning = config['use_automatic_entropy_tuning']
        self.cql_n_actions = config['cql_n_actions']
        self.cql_importance_sample = config['cql_importance_sample']
        self.cql_lagrange = config['cql_lagrange']
        self.lagrange_threshold = config['lagrange_threshold']
        self.cql_temp = config['cql_temp']
        self.cql_alpha = config['cql_alpha']
        self.cql_max_target_backup = config['cql_max_target_backup']
        self.alpha_multiplier = 1.0
        self.cql_weight = config['cql_weight']
        
        if self.use_automatic_entropy_tuning:
            self.log_alpha = Scalar(0.0)
            self.alpha_optimizer = torch.optim.Adam(
                self.log_alpha.parameters(),
                lr=self.alpha_lr,
            )
            self.alpha = self.log_alpha.detach().exp()
        else:
            self.log_alpha = None

        self.log_alpha_prime = Scalar(1.0)
        self.alpha_prime_optimizer = torch.optim.Adam(
            self.log_alpha_prime.parameters(),
            lr=self.alpha_lr,
        )
        
        # Initialize wandb logging
        run_name = f"DT-CORL-loaded-D4RL_{config['env']}_Delay_{config['delay']}_Seed_{config['seed']}_ID_{uuid.uuid4()}"
        group_name = f'DT-CORL-loaded'
        if self.wandb:
            wandb.init(
                # project="Offline_Delayed_RL", 
                project="OfflineRL_Trans",
                name=run_name,
                group=group_name,
                config=config,
                sync_tensorboard=True,
            )
        # Initialize the log_dict for logging the training process and seed
        seed = config['seed']
        np.random.seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)  
        self.log_metric = {}


    def update_target_networks(self, soft_update_factor):
        for param, target_param in zip(self.critic_1.parameters(), self.target_1.parameters()):
            target_param.data.copy_(soft_update_factor * param.data + (1 - soft_update_factor) * target_param.data)
        for param, target_param in zip(self.critic_2.parameters(), self.target_2.parameters()):
            target_param.data.copy_(soft_update_factor * param.data + (1 - soft_update_factor) * target_param.data)


    def train_dynamic(self):
        self.belief_buffer.generate_sample_prior()
        for indices in self.belief_buffer._sample_prior:
            states, actions, rewards, dones, masks = self.belief_buffer.sample(indices)
            states = states.to(self.device)
            actions = actions.to(self.device)
            rewards = rewards.to(self.device)
            masks = masks[:, 1:, 0].to(self.device)

            latents = self.auto_encoder.encode(states)
            timesteps = torch.arange(0, self.config['delay'], dtype=torch.int32).to(self.device)
            z = self.latent_dynamic(latents=latents[:, :1, :], 
                             actions=actions[:, :self.config['delay'], :],
                             rewards=rewards[:, :self.config['delay'], :],
                             timesteps=timesteps,
                             masks=masks)
            rec_states = self.auto_encoder.decode(z)
            loss = F.mse_loss(rec_states, states[:, 1:, :], reduction='none').mean(-1)
            loss = ((1-masks) * loss).mean()
            self.dynamic_optimizer.zero_grad()
            loss.backward()
            self.dynamic_optimizer.step()
        self.log_metric["dynamic_loss"] = loss.item()


    def rollout(self, num_rollout):
        traj_dict = defaultdict(list)
        for _ in range(num_rollout):
            # indices = random.choice(self.belief_buffer._sample_prior)
            belief_states, belief_actions, belief_rewards, belief_dones, belief_masks = self.belief_buffer.sample(batch_size=1)
            belief_states = belief_states.to(self.device)
            belief_actions = belief_actions.to(self.device)
            belief_rewards = belief_rewards.to(self.device)
            belief_masks = belief_masks[:, 1:, 0].to(self.device)
            with torch.no_grad():
                latents = self.auto_encoder.encode(belief_states)
                timesteps = torch.arange(0, self.config['delay'], dtype=torch.int32).to(self.device)
                pred_latents = self.latent_dynamic(latents=latents[:, :1, :], 
                                actions=belief_actions[:, :self.config['delay'], :],
                                rewards=belief_rewards[:, :self.config['delay'], :],
                                timesteps=timesteps,
                                masks=belief_masks)
                pred_states = self.auto_encoder.decode(pred_latents)
            curr_obs = belief_states[:, 0, :]
            for i in range(self.config['delay']):
                action = belief_actions[:, i, :]
                next_obs = pred_states[:, i, :]
                reward = belief_rewards[:, i, :]
                done = belief_dones[:, i, :]
                self.synthetic_data_buffer.store(curr_obs, action, reward, next_obs, done)
                traj_dict['action'].append(belief_actions[:, i, :])
                traj_dict['obs'].append(curr_obs)
                traj_dict['next_obs'].append(pred_states[:, i, :])
                traj_dict['reward'].append(belief_rewards[:, i, :])
                traj_dict['done'].append(belief_dones[:, i, :])
                curr_obs = next_obs
        errors = F.l1_loss(belief_states[:, 1:, :], pred_states, reduction='none').mean(-1)
        errors = ((1 - belief_masks) * errors).mean()
        breakpoint()
        self.log_metric["rollout_error"] = errors.item()
        return traj_dict 


    def train(self, real_batch, synthetic_batch):
        mix_batch = {k: torch.cat([real_batch[k], synthetic_batch[k]], 0) for k in real_batch.keys()}
        obss, actions, next_obss, rewards, dones = mix_batch["obss"], mix_batch["actions"], \
            mix_batch["next_obss"], mix_batch["rewards"], mix_batch["dones"]
        batch_size = obss.shape[0]
        # update actor
        new_action, log_prob, _ = self.actor.get_action(obss)
        q1a, q2a = self.critic_1(obss, new_action), self.critic_2(obss, new_action)
        actor_loss = (self.alpha * log_prob - torch.min(q1a, q2a)).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        if self.use_automatic_entropy_tuning:
            log_prob = log_prob.detach() + self.target_entropy
            alpha_loss = -(self.log_alpha * log_prob).mean()
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            self.alpha = self.log_alpha.detach().exp()

        # compute td error
        if self.cql_max_target_backup:
            with torch.no_grad():
                tmp_next_obss = next_obss.unsqueeze(1) \
                    .repeat(1, self.cql_n_actions, 1) \
                    .view(batch_size * self.cql_n_actions, next_obss.shape[-1])
                tmp_next_actions, _ = self.actor.get_action(tmp_next_obss)
                tmp_next_q1 = self.target_1(tmp_next_obss, tmp_next_actions) \
                    .view(batch_size, self.cql_n_actions, 1) \
                    .max(1)[0].view(-1, 1)
                tmp_next_q2 = self.target_2(tmp_next_obss, tmp_next_actions) \
                    .view(batch_size, self.cql_n_actions, 1) \
                    .max(1)[0].view(-1, 1)
                next_q = torch.min(tmp_next_q1, tmp_next_q2)
        else:
            with torch.no_grad():
                next_actions, next_log_probs, _ = self.actor.get_action(next_obss)
                next_q = torch.min(
                    self.target_1(next_obss, next_actions),
                    self.target_2(next_obss, next_actions)
                )
                if self.backup_entropy:
                    next_q -= self.alpha * next_log_probs
        
        td_target = rewards + (1. - dones) * self.discount * next_q
        qf1_loss = F.mse_loss(self.critic_1(obss, actions), td_target)
        qf2_loss = F.mse_loss(self.critic_2(obss, actions), td_target)

        # compute conservative loss
        random_actions = torch.FloatTensor(
            batch_size * self.cql_n_actions, actions.shape[-1]
        ).uniform_(self.env.action_space.low[0], self.env.action_space.high[0]).to(self.device)
        tmp_obss = obss.unsqueeze(1) \
            .repeat(1, self.cql_n_actions, 1) \
            .view(batch_size * self.cql_n_actions, obss.shape[-1])
        tmp_next_obss = next_obss.unsqueeze(1) \
            .repeat(1, self.cql_n_actions, 1) \
            .view(batch_size * self.cql_n_actions, obss.shape[-1])
        with torch.no_grad():
            tmp_curr_actions, tmp_curr_log_probs, _ = self.actor.get_action(tmp_obss)
            tmp_next_actions, tmp_next_log_probs, _ = self.actor.get_action(tmp_next_obss)
        q1_rand = self.critic_1(tmp_obss, random_actions)
        q2_rand = self.critic_2(tmp_obss, random_actions)
        q1_curr = self.critic_1(tmp_obss, tmp_curr_actions)
        q2_curr = self.critic_2(tmp_obss, tmp_curr_actions)
        q1_next = self.critic_1(tmp_next_obss, tmp_next_actions)
        q2_next = self.critic_2(tmp_next_obss, tmp_next_actions)

        for q in [q1_rand, q2_rand, q1_curr, q2_curr, q1_next, q2_next]:
            q = q.view(batch_size, self.cql_n_actions, 1)
        cat_q1 = torch.cat([q1_curr, q1_next, q1_rand], 1)
        cat_q2 = torch.cat([q2_curr, q2_next, q2_rand], 1)
        real_obss, real_actions = real_batch["obss"], real_batch["actions"]
        real_q1 = self.critic_1(real_obss, real_actions)
        real_q2 = self.critic_2(real_obss, real_actions)

        conservative_loss1 = torch.logsumexp(cat_q1 / self.cql_temp, 1).mean() * self.cql_weight * self.cql_temp - \
            real_q1.mean() * self.cql_weight
        conservative_loss2 = torch.logsumexp(cat_q2 / self.cql_temp, 1).mean() * self.cql_weight * self.cql_temp - \
            real_q2.mean() * self.cql_weight
        
        if self.cql_lagrange:
            cql_alpha = torch.clamp(self.log_alpha_prime.exp(), 0.0, 1e6)
            conservative_loss1 = cql_alpha * (conservative_loss1 - self.lagrange_threshold)
            conservative_loss2 = cql_alpha * (conservative_loss2 - self.lagrange_threshold)

            self.alpha_prime_optimizer.zero_grad()
            cql_alpha_loss = -(conservative_loss1 + conservative_loss2).mean()
            cql_alpha_loss.backward(retain_graph=True)
            self.alpha_prime_optimizer.step()

        critic_loss1 = qf1_loss + conservative_loss1
        critic_loss2 = qf2_loss + conservative_loss2

        # update critic
        self.critic_1_optimizer.zero_grad()
        critic_loss1.backward()
        self.critic_1_optimizer.step()

        self.critic_2_optimizer.zero_grad()
        critic_loss2.backward()
        self.critic_2_optimizer.step()

        self.update_target_networks(self.soft_update_factor)
        self.log_metric["actor_loss"] = actor_loss.item()
        self.log_metric["critic_loss1"] = critic_loss1.item()
        self.log_metric["critic_loss2"] = critic_loss2.item()


    def get_next_latent_trans_dy(self, next_latent, actions, rewards):
        delayed_idx = len(actions) - 1
        next_latent = next_latent.unsqueeze(0)
        # timesteps = torch.arange(0, len(actions), dtype=torch.int32).to(self.config['device'])
        timesteps = torch.arange(0, self.config['delay'], dtype=torch.int32).to(self.config['device'])
        masks = torch.zeros(len(actions)).unsqueeze(0).to(self.config['device'])
        pad_masks = torch.ones(self.config['delay'] - len(actions)).unsqueeze(0).to(self.config['device'])
        masks = torch.concat((masks, pad_masks), dim=-1)
        action_dim = actions[0].shape[0]
        pad_actions = torch.zeros((1, self.config['delay'] - len(actions), action_dim)).to(self.config['device'])
        pad_rewards = torch.zeros((1, self.config['delay'] - len(actions))).to(self.config['device'])
        next_latent = self.latent_dynamic(
            latents=next_latent, 
            actions=torch.concat((torch.FloatTensor(np.array(list(actions))).unsqueeze(0).to(self.config['device']), pad_actions), dim=1),
            rewards=torch.concat((torch.FloatTensor(np.array(list(rewards))).unsqueeze(0).to(self.config['device']), pad_rewards), dim=1).unsqueeze(-1),
            timesteps=timesteps,
            masks=masks
        )
        next_latent = next_latent[:, delayed_idx, :].squeeze(0)
        return next_latent


    def evaluate(self):
        self.log_metric['eval_r'] = []
        self.log_metric['eval_l'] = []
        self.auto_encoder.eval(), self.latent_dynamic.eval(), self.actor.eval()
    
        for i in trange(self.config['num_evaluation']):
            obs, done = self.eval_env.reset(), 0
            delayed_deque = {
                'obs': deque(maxlen=self.config['delay'] + 1),
                'action': deque(maxlen=self.config['delay']),
                'reward': deque(maxlen=self.config['delay']),
            }
            delayed_deque['obs'].append(torch.FloatTensor(obs).to(self.config['device']))
            rec_obs = torch.FloatTensor(obs).to(self.config['device'])
            while not done:
                with torch.no_grad():
                    action, _, _ = self.actor.get_action(rec_obs)
                    action = action.squeeze().cpu().numpy()
                
                next_obs, reward, done, info = self.eval_env.step(action)

                delayed_deque['obs'].append(torch.FloatTensor(next_obs).to(self.config['device']))
                delayed_deque['action'].append(action)
                delayed_deque['reward'].append((reward-self.reward_mean.item())/self.reward_std.item())

                # estimate next latent
                with torch.no_grad():
                    next_latent = self.auto_encoder.encode(delayed_deque['obs'][0])
                    next_latent = self.get_next_latent_trans_dy(next_latent, delayed_deque['action'], delayed_deque['reward'])
                    next_rec_obs = self.auto_encoder.decode(next_latent)

                # latent = next_latent
                obs = next_obs
                rec_obs = next_rec_obs
                if done:
                    self.log_metric['eval_r'].append(info['episode']['r'])
                    self.log_metric['eval_l'].append(info['episode']['l'])

        eval_score = np.mean(self.log_metric['eval_r'])
        normalized_eval_score = self.env.get_normalized_score(eval_score)          
        self.log_metric['eval_r'] = normalized_eval_score
        self.log_metric['eval_l'] = np.mean(self.log_metric['eval_l'])
        print(f"Eval Reward: {self.log_metric['eval_r']}, Eval Length: {self.log_metric['eval_l']}")

        self.auto_encoder.train(), self.latent_dynamic.train(), self.actor.train()


    def logging(self):
        for k in self.log_metric.keys():
            self.logger.add_scalar(k, self.log_metric[k])
        if self.wandb:
            wandb.log(self.log_metric)
        self.log_metric = {}
         

    def state_dict(self) -> Dict[str, Any]:
        return {
            "actor": self.actor.state_dict(),
            # "critic1": self.critic_1.state_dict(),
            # "critic2": self.critic_2.state_dict(),
            # "critic1_target": self.target_1.state_dict(),
            # "critic2_target": self.target_2.state_dict(),
            # "critic_1_optimizer": self.critic_1_optimizer.state_dict(),
            # "critic_2_optimizer": self.critic_2_optimizer.state_dict(),
            "actor_optim": self.actor_optimizer.state_dict(),
            # "sac_log_alpha": self.log_alpha,
            # "sac_log_alpha_optim": self.alpha_optimizer.state_dict(),
            # "cql_log_alpha": self.log_alpha_prime,
            # "cql_log_alpha_optim": self.alpha_prime_optimizer.state_dict(),
            "auto_encoder": self.auto_encoder.state_dict(),
            "latent_dynamic": self.latent_dynamic.state_dict(),
            "dynamic_optimizer": self.dynamic_optimizer.state_dict(),
        }

    def load_models(self, path):
        checkpoint = torch.load(f"{path}/models.pth", map_location=torch.device('cpu'))
        self.auto_encoder.load_state_dict(checkpoint['auto_encoder'])
        self.auto_encoder.eval()
        print('loaded auto encoder')
        self.latent_dynamic.load_state_dict(checkpoint['latent_dynamic'])
        self.latent_dynamic.eval()
        print('loaded latent dynamic')
        self.actor.load_state_dict(checkpoint['actor'])
        self.actor.eval()
        print('loaded actor')
    
    def load_dyn_models(self, checkpoint):
        self.auto_encoder.load_state_dict(checkpoint['auto_encoder'])
        self.auto_encoder.eval()
        print('loaded auto encoder')
        self.latent_dynamic.load_state_dict(checkpoint['latent_dynamic'])
        self.latent_dynamic.eval()
        print('loaded latent dynamic')
    
    
if __name__ == "__main__":
    configs = {
        "env": ["hopper-medium-v2"],
        "device":  ["cuda" if torch.cuda.is_available() else "cpu"],
        "seed": [1, 2, 3],  # Sets Gym, PyTorch and Numpy seeds
        # "seed": [1],
        "gamma": [0.99],
        "alpha": [0.2],
        "total_step": [int(1e6)],  # Max time steps to run environment
        "buffer_size": [int(1e6)],  # Replay buffer size
        "batch_size": [256],  # Batch size for all networks
        "lr_actor": [3e-4],
        "lr_critic": [3e-4],
        "lr_dynamic": [3e-4],
        "lr_alpha": [1e-3],
        "latent_dim": [256],
        "num_layers": [10],
        "num_heads": [4],
        "attention_dropout": [0.1],
        "residual_dropout": [0.1],
        "hidden_dropout": [0.1],
        "soft_update_factor": [5e-3],
        "learn_start": [int(1e2)],
        "rollout_freq": [int(1e3)],
        "num_rollout": [int(1e3)],
        "evaluate_freq": [int(1e4)],
        "num_evaluation": [10],
        "delay": [int(8)],
        "dynamic_type": ['trans'],
        "use_automatic_entropy_tuning": [False],
        "cql_n_actions": [10],
        "cql_importance_sample": [True],
        "cql_lagrange": [False],
        "cql_target_action_gap": [-1.0],
        "lagrange_threshold": [10.0],
        "cql_temp": [1.0],
        "cql_alpha": [0.2],
        "wandb": [True],
        "backup_entropy": [False],
        "cql_max_target_backup": [False],
        "cql_weight": [1.0],
        "fake_real_ratio": [1.0],
        "dynamic_train_threshold": [2e5],
    }
    configs = get_configs(configs)
    for config in configs:
        
        config['exp_tag'] = f"logs/dt-corl/{config['env']}/{config['delay']}/SEED_{config['seed']}"
        # if os.path.exists(config['exp_tag']):
        #     continue
        #Test Initialization
        trainer = DTOCRL(config)
        
        
        for i in trange(config['total_step']):
            # trainer.train_dynamic()
            if i % config['rollout_freq'] == 0:
                _, error = trainer.rollout(config['num_rollout'])

            real_data = trainer.replay_buffer.sample(batch_size=trainer.config['batch_size'])
            # real_data = trainer.synthetic_data_buffer.sample(batch_size=trainer.config['batch_size'], device=trainer.device)
            # synthetic_data = trainer.replay_buffer.sample(batch_size=trainer.config['batch_size'])
            synthetic_data = trainer.synthetic_data_buffer.sample(batch_size=int(
                trainer.config['batch_size']*trainer.config['fake_real_ratio']), device=trainer.device)
            real_batch = construct_dict_from_tuple(real_data, trainer.config['device'])
            synthetic_batch = construct_dict_from_tuple(synthetic_data, trainer.config['device'])
            trainer.train(real_batch, synthetic_batch)
            if i % config['evaluate_freq'] == 0:
                trainer.evaluate()
                state_dict = trainer.state_dict()
                torch.save(state_dict, f"{config['exp_tag']}/models.pth")
            trainer.logging()
        wandb.finish()
        trainer.logger.close()
        print(f'Finished training for Seed{config["seed"]}-{config["env"]}')
        print(f'======================================================================')
        
        
        
    